{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Segmentation dataset prepration"
      ],
      "metadata": {
        "id": "0lqeQ-BeLMLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from zipfile import ZipFile \n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "MH_PMQmKCf7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to store our dataset in a pandas dataframe for fast and accurate process. Then we will convert these dataframe to a Tensor. so first let's define our helper functions"
      ],
      "metadata": {
        "id": "OpEiOX1qpadn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_creation(image_path, name):\n",
        "    \"\"\"\n",
        "    This function is storing path of the images in a dataframe beside of each image id (in image name).\n",
        "    Walk though the image_path and read the dirpathes and images name in each dir.\n",
        "    Then append each image full path in a list.\n",
        "    Extract the image name without the full path and extension and append it to the ids list.\n",
        "    Please make sure each full path in first list is correspond to the id in second list at the same index.\n",
        "    Arguments:\n",
        "      image_path: root directory full path\n",
        "      name: name for column of full pathes in dataframe\n",
        "    return:\n",
        "      df: a df contains ids and full path of each image id. call the ids column, id and pathes column, name.\n",
        "          Please set the ids column to be index in this df.\n",
        "\n",
        "    \"\"\"\n",
        "    ##################################################\n",
        "    ############### YOUR CODES GO HERE ###############\n",
        "\n",
        "    df = ....\n",
        "\n",
        "    return df\n",
        "    ##################################################"
      ],
      "metadata": {
        "id": "Csjy05ZmCgwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        # a function for showing images\n",
        "        # please write a code to convert images from tensor to ordinary images\n",
        "        # use https://docs.w3cub.com/tensorflow~2.3/keras/preprocessing/image/array_to_img\n",
        "        #### Your Code goes here\n",
        "        ....\n",
        "        ####\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qiowJyj2DK12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating train and train_masks directories for stroring the processed images"
      ],
      "metadata": {
        "id": "OD3Rodztul5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train\n",
        "!mkdir train_masks"
      ],
      "metadata": {
        "id": "ng_3pjWrDMs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/ss_dataset'\n",
        "img_size = 256\n",
        "\n",
        "image_root = '/content/train'\n",
        "label_root = '/content/train_masks'\n",
        "\n",
        "if not os.path.isdir(image_root):\n",
        "    os.mkdir(image_root)\n",
        "if not os.path.isdir(label_root):\n",
        "    os.mkdir(label_root)\n",
        "\n",
        "images = list()\n",
        "labels = list()\n",
        "\n",
        "\"\"\"\n",
        "Beacause your images stored beside correspond label, and their format is in .bmp, so\n",
        "please first store path of all images in a common list.\n",
        "Then separate labels (which have _label pattern in their name) from images and store them in \n",
        "images and labels lists.\n",
        "For each image and label file, read it with PIL image lib. and resize them to 256*256.\n",
        "Then convert them to png file\n",
        "At last save them in image_root and label_root directories in a way none of the files with the same name are lost\n",
        "for reading images:\n",
        "    https://www.geeksforgeeks.org/python-pil-image-open-method/\n",
        "for saving images:\n",
        "    https://www.geeksforgeeks.org/python-pil-image-save-method/\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "################################################\n",
        "############## YOUR CODES GO HERE ##############\n",
        "################################################"
      ],
      "metadata": {
        "id": "iTReo6A1DX4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set:  \", len(os.listdir(\"/content/train\")))\n",
        "print(\"Train masks:\", len(os.listdir(\"/content/train_masks\")))"
      ],
      "metadata": {
        "id": "BRPX2p5ADru1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Call the dataframe_creation function with apprieciate arguments to get a \n",
        "## suitable dataframes for images and masks. Call the images pathes column's name \"image_path\"\n",
        "## and column for masks name \"mask_name\"\n",
        "## In the end, create a new column in df of images called \"mask_path\" and fill it with \"mask_path\" column of the mask's df\n",
        "################################################\n",
        "############## YOUR CODES GO HERE ##############\n",
        "################################################"
      ],
      "metadata": {
        "id": "LH1DOcCODzDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = [256, 256]\n",
        "\n",
        "def data_augmentation(img, mask_img):\n",
        "    \"\"\"\n",
        "    A function for data augmentation.\n",
        "    We wanna just do some flips.\n",
        "    Just make a random number, if it was greater than 0.5 do a lef_right flip\n",
        "    hint:\n",
        "      https://www.tensorflow.org/api_docs/python/tf/random/uniform\n",
        "      https://www.tensorflow.org/api_docs/python/tf/image/flip_left_right\n",
        "    Arguments:\n",
        "      img: image tensor\n",
        "      mask_img: mask image tensor\n",
        "    return:\n",
        "      img, mask_img\n",
        "    \"\"\"\n",
        "\n",
        "    ################################################\n",
        "    ############## YOUR CODES GO HERE ##############\n",
        "    ....\n",
        "    return img, mask_img\n",
        "    ################################################\n",
        "\n",
        "def preprocessing(path, mask_path):\n",
        "    '''\n",
        "    Do the usual preprocessing steps for image processing algorithms\n",
        "    Read image tensors. decode them, resize to img_size, cast them fo float dtype and normalize between 0-1\n",
        "    Hint:\n",
        "      https://www.tensorflow.org/api_docs/python/tf/io/read_file\n",
        "      https://www.tensorflow.org/api_docs/python/tf/io/decode_jpeg\n",
        "      https://www.tensorflow.org/api_docs/python/tf/image/resize\n",
        "      https://www.tensorflow.org/api_docs/python/tf/cast\n",
        "    Set channels in decoding to 3\n",
        "    Arguments:\n",
        "      path: image path\n",
        "      mask_path: mask image path\n",
        "    return:\n",
        "      pre_processed image and mask image tensors\n",
        "    '''\n",
        "    ################################################\n",
        "    ############## YOUR CODES GO HERE ##############\n",
        "    ....\n",
        "    return img, mask_img\n",
        "    ################################################\n",
        "\n",
        "def create_dataset(df, train = False):\n",
        "    '''\n",
        "    A function for applying preprocessing and augmentation steps.\n",
        "    Augment data just in train mode.\n",
        "    First make a Dataset of tensors to reach high speed and ability.\n",
        "    Then apply needed steps.\n",
        "    For creating dataset, please use tensorflow-tf-data-dataset-from_tensor_slices to get\n",
        "      a dataset of images and correspondig masks path. use values of image_path and mask_path columns of your dataframe\n",
        "    Then use map function of created ds and call above functions respectively.\n",
        "    use tf.data.AUTOTUNE in map function\n",
        "    Hint:\n",
        "      https://www.geeksforgeeks.org/tensorflow-tf-data-dataset-from_tensor_slices/\n",
        "      https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    Arguments:\n",
        "      df: dataframe of images with masks path.\n",
        "      train: boolean for switching between train and inference mode.\n",
        "    return:\n",
        "      dataset\n",
        "    '''\n",
        "    ################################################\n",
        "    ############## YOUR CODES GO HERE ##############\n",
        "    ....\n",
        "    return ds\n",
        "    ################################################"
      ],
      "metadata": {
        "id": "NP0QGQu7EJLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### first split the dataframe to train and val (with train_test_split) then create dataset from each df\n",
        "train_df, valid_df = ....\n",
        "train = ...\n",
        "valid = ..."
      ],
      "metadata": {
        "id": "q3UQwoV3ENYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_LENGTH = len(train_df)\n",
        "BATCH_SIZE = 24\n",
        "BUFFER_SIZE = 1000"
      ],
      "metadata": {
        "id": "wwG_qNpfERm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The last step for creating a tf.data.Dataset is to batch and shuffle them\n",
        "## for this please cache train dataset, then shuffle in the amount of BUFFER_SIZE, then \n",
        "## batch them in the amount of BATCH_SIZE and repeat this process. Finally prefetch the train_data with \n",
        "## buffer_size=tf.data.AUTOTUNE (https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
        "## Hint: name of needed functions are in the description, just call\n",
        "## for valid_dataset, just batch in the amount of BATCH_SIZE\n",
        "################################################\n",
        "############## YOUR CODES GO HERE ##############\n",
        "train_dataset = ...\n",
        "train_dataset = ...\n",
        "valid_dataset = ...\n",
        "################################################"
      ],
      "metadata": {
        "id": "gE6rIsOJEUYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's see some of the images ans masks\n",
        "for i in range(5):\n",
        "    for image, mask in train.take(i):\n",
        "        sample_image, sample_mask = image, mask\n",
        "        display([sample_image, sample_mask])"
      ],
      "metadata": {
        "id": "ACTUhmuJEYi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now we created our dataset, let's implement the model and train it on created dataset"
      ],
      "metadata": {
        "id": "galjxE9v7EL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we're going to implement [U-Net](https://arxiv.org/abs/1505.04597) model for semantic segmentation.\n",
        "For its backbone, we'll use [MobileNetV2](https://arxiv.org/abs/1801.04381) architecture."
      ],
      "metadata": {
        "id": "5srHPEH98TLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net model for segmentation"
      ],
      "metadata": {
        "id": "C_Z2tVicLSvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "First load the pre-trained weights of mobilenetv2. Because amount of our data is limited its better to use \n",
        "transfer learning. Set the input_shape of the model to img_size, and please don't include the top of this model.\n",
        "According to U-Net architecture, because we have skip connections in multiple steps of backbone, please \n",
        "get the below layers's endpoint from backbone:\n",
        "    block_1_expand_relu   # 64x64\n",
        "    block_3_expand_relu   # 32x32\n",
        "    block_6_expand_relu   # 16x16\n",
        "    block_13_expand_relu  # 8x8\n",
        "    block_16_project\n",
        "Store them in a list.\n",
        "Create your model with mobilenetv2 input layer and extracted output layers.\n",
        "Freeze backbone and don't train it.\n",
        "Hint:\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/MobileNetV2\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "\"\"\"\n",
        "\n",
        "################################################\n",
        "############## YOUR CODES GO HERE ##############\n",
        "################################################"
      ],
      "metadata": {
        "id": "5-sL1LKaEyOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(filters, size):\n",
        "    '''\n",
        "    in this function we have to define the decoder section of the u-net model.\n",
        "    we call it upsampling process.\n",
        "    for each down-sampling in the backbone, we have a up-sampling in decoder.\n",
        "    we should implement decoder in a way to double the spatial resolution of the input tensor.\n",
        "    bacause the inputs, are our features, we should learn how to double the spatial resolution instead of\n",
        "    resizing methods for ignoring inserting fake features to our feature vectors.\n",
        "    it's a crucial statement.\n",
        "    so use a transpose conv2d function and double spatial resolution of the input tensor\n",
        "    this layer should get the filters as output channel, size for filter size, 2 for stride.\n",
        "    use same padding in it.\n",
        "    define the initializer for kernel initializer and use False for use_bias.\n",
        "    Use a sequential model for stacking layers.\n",
        "    Finally add a batchnormalization layer at the end of the model. (use dafult args)\n",
        "    Hint:\n",
        "      https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "      https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\n",
        "      https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
        "    Atguments:\n",
        "      filters: number of input channels\n",
        "      size: spatial dimention of filters\n",
        "    return:\n",
        "      decoder layers\n",
        "    '''\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    \n",
        "    ################################################\n",
        "    ############## YOUR CODES GO HERE ##############\n",
        "    result = ....\n",
        "    \n",
        "\n",
        "    ################################################\n",
        "\n",
        "    return result\n",
        "\n",
        "up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "metadata": {
        "id": "uwN3neHqE4B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(output_channels):\n",
        "    '''\n",
        "    Now we have to stack backbone with decoder.\n",
        "    First define a Input layer to get the images (consider the img_size).\n",
        "    Then give the result to the backbone and get the outputs of this encoder.\n",
        "    you will get a list of outputs.\n",
        "    store the last element of this list for input of the decoder.\n",
        "    Reverse the outputs list except last element and keep them as skip connections.\n",
        "    now iterate up_stack and skips lists simultaneously, give the previous last feature vector to the\n",
        "    decoder part and get the output. concat that with current skip from backbone and name it as same as\n",
        "    the last output to make it input of the next decoder layer.\n",
        "    Finally, use a transpose conv2d layer to get the final coarse map.\n",
        "    this layer should get the output_channels as output channel, 3 for filter size, 2 for stride, sigmoid for activation function.\n",
        "    use same padding in it.\n",
        "    give the last output of the decor to it and use as your model's main output.\n",
        "    Hint:\n",
        "      http://man.hubwiz.com/docset/TensorFlow.docset/Contents/Resources/Documents/api_docs/python/tf/keras/layers/Input.html\n",
        "      https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate\n",
        "      https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\n",
        "      https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "    '''\n",
        "    ################################################\n",
        "    ############## YOUR CODES GO HERE ##############\n",
        "    \n",
        "    return ...\n",
        "    ################################################"
      ],
      "metadata": {
        "id": "al3IACREE6W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's define the loss function. dice loss function is a good choice for semantic segmenation task\n",
        "## See https://dev.to/_aadidev/3-common-loss-functions-for-image-segmentation-545o\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
        "\n",
        "def dice_loss(in_gt, in_pred):\n",
        "    return 1-dice_coef(in_gt, in_pred)\n",
        "\n",
        "## define model with  output channel. Name it model.\n",
        "## then compile the model with adam optimizer, dice loss\n",
        "## please introduce binary_accuracy and dice coef to the model to log them as a metric during training\n",
        "## hint: https://keras.io/api/models/model_training_apis/\n",
        "\n",
        "############################################\n",
        "######### YOUR CODES GO HERE ###############\n",
        "############################################\n",
        "\n",
        "## plot the graph of the model with each layer's shape\n",
        "## hint: https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\n",
        "\n",
        "\n",
        "############################################\n",
        "######### YOUR CODE GOES HERE ###############\n",
        "############################################"
      ],
      "metadata": {
        "id": "ya0ch2wzE_DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, masks in train_dataset.take(1):\n",
        "    for img, mask in zip(images, masks):\n",
        "        sample_image = img\n",
        "        sample_mask = mask\n",
        "        break\n",
        "\n",
        "def visualize(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_predictions(sample_image, sample_mask):\n",
        "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
        "    pred_mask = pred_mask.reshape(img_size[0],img_size[1],1)\n",
        "    visualize([sample_image, sample_mask, pred_mask])\n",
        "    \n",
        "\n",
        "## let's see some the outputs of pre-trained model\n",
        "show_predictions(sample_image, sample_mask)"
      ],
      "metadata": {
        "id": "_WMJiyDHFCAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "dTifBoQhFKG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### first define a early stop callback for preventing overfitting.\n",
        "### set patience=4 and True the restore_best_weight \n",
        "### Hint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
        "\n",
        "############################################\n",
        "######### YOUR CODE GOES HERE ###############\n",
        "############################################\n",
        "\n",
        "# Let's observe how the model improves while it is training. \n",
        "# To accomplish this task, a callback function is defined below.\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            show_predictions(sample_image, sample_mask)\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "### Define the STEPS_PER_EPOCH and fit the model\n",
        "### use train_dataset and EPOCHS and STEPS_PER_EPOCH for training process\n",
        "### use valid_dataset for validation and introduce callbacks to the model\n",
        "### save history\n",
        "\n",
        "############################################\n",
        "######### YOUR CODES GO HERE ###############\n",
        "############################################"
      ],
      "metadata": {
        "id": "1DVAT1iSFRTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### let's see some the new predictions\n",
        "for i in range(5):\n",
        "    for image, mask in valid.take(i):\n",
        "        sample_image, sample_mask = image, mask\n",
        "        show_predictions(sample_image, sample_mask)"
      ],
      "metadata": {
        "id": "u-UbcGmfFStQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### save final checkpoints to the disk\n",
        "### Use h5 extension\n",
        "### Hint: https://www.tensorflow.org/guide/keras/save_and_serialize\n",
        "\n",
        "############################################\n",
        "######### YOUR CODES GO HERE ###############\n",
        "############################################"
      ],
      "metadata": {
        "id": "GUmD6uXSF7RS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}